{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf100
{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\csgray\c0;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs22 \cf2 \CocoaLigature0 Lab 4\
\
GMM_EM.py and GMM-NN.py can be run separately.\
\
*GMM-EM\
\
The Expectation-Maximization algorithm was implemented to fit data to a Gaussian Mixture Model. The mean and variance is randomly initialized. The following results were obtained:-\
\
Enter random data point between (-3, 8): 7\
Predicted Probabilities:  [2.30733612e-099 8.91615312e-001 3.19561732e-140 1.08384688e-001\
 9.75823240e-180]\
\
*GMM-NN\
\
A model was trained to classify data points to the different gaussians. The following model was obtained:-\
_________________________________________________________________\
Layer (type)                 Output Shape              Param #   \
=================================================================\
input_layer (InputLayer)     (None, 1)                 0         \
_________________________________________________________________\
batch_normalization_1 (Batch (None, 1)                 4         \
_________________________________________________________________\
dropout_1 (Dropout)          (None, 1)                 0         \
_________________________________________________________________\
layer_1 (Dense)              (None, 200)               400       \
_________________________________________________________________\
batch_normalization_2 (Batch (None, 200)               800       \
_________________________________________________________________\
dropout_2 (Dropout)          (None, 200)               0         \
_________________________________________________________________\
layer_2 (Dense)              (None, 200)               40200     \
_________________________________________________________________\
batch_normalization_3 (Batch (None, 200)               800       \
_________________________________________________________________\
dropout_3 (Dropout)          (None, 200)               0         \
_________________________________________________________________\
layer_3 (Dense)              (None, 200)               40200     \
_________________________________________________________________\
batch_normalization_4 (Batch (None, 200)               800       \
_________________________________________________________________\
dropout_4 (Dropout)          (None, 200)               0         \
_________________________________________________________________\
output_layer (Dense)         (None, 4)                 804       \
=================================================================\
Total params: 84,008\
Trainable params: 82,806\
Non-trainable params: 1,202\
_________________________________________________________________}